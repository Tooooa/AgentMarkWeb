# 2 总体方案设计

本章强调产品级系统视角，我们不仅实现行为水印算法 AgentMark、AgentMark-F，还围绕真实 Agent 运行链路设计了可落地的端到端系统，覆盖水印注入、日志采集、鲁棒溯源、可视化监管四个闭环环节。

本方案的核心思想是规划行为层水印、分布保留采样、抗擦除编码，并结合本项目现有实现，包括 `agentmark` 算法库、`agentmark\proxy` 网关代理、`dashboard` 监管大屏，系统整体围绕三层架构展开。感知层涵盖 Chrome 插件与网关形态，负责拦截 Agent 请求、注入水印指令、捕获行为日志与上下文；算法层为 Core Logic，基于 Distribution-Preserving Sampling 的规划行为采样与 Erasure-Resilient Coding，并提供 RLNC 等鲁棒解码能力；应用层为前端 Dashboard，负责解码验证、统计检验、攻击链还原与审计展示。

---

## 2.1 设计原则

> 本文的算法记号统一采用数学排版。每一步的候选规划行为集合记为 B_t，显式行为分布记为 P_t。分布保留采样在 P_t 上输出带水印的规划行为 \hat{b}_t，工程字段名常写作 `b_wm_t`。整体载荷比特串记为 m，每一步嵌入的变长比特子串记为 m_t，验证侧从观测序列恢复 \hat{m}_t 并在多步聚合后得到 \hat{m}。同步所需的每步上下文记为 Context_t，工程字段名常写作 `context`。

### 2.1.1 非侵入性，不修改大模型权重
目标是可在黑盒 API 与第三方 Agent 生态中部署，不要求重新训练或改动模型参数，也不要求修改底层推理引擎的 token 采样过程。同时尽量不改变业务方既有的 Agent 结构、工具协议与调用链路，使水印能力可作为旁路能力以最低成本上线、下线。
从工程角度看，非侵入性不仅是不改权重的要求，更是产品落地的可运维性要求。一套系统想进入真实业务链路，必须做到可插拔、可回滚、可跨模型、跨框架复用，并在失败时能够降级为无水印但不影响任务完成。
非侵入性通过三层设计来落地，分别对应介入点选择、证据表达方式、接入形态的可插拔性。
首先，系统把水印施加在规划行为的选择上，而不是 token 生成过程。我们将 Agent 决策拆为规划行为与执行动作两层，规划行为决定做什么，执行动作决定怎么做，水印只作用于规划行为的概率采样阶段，后续工具调用参数生成与内容生成逻辑保持原样。这样做的收益是工程可控，既能保持工具调用语义与安全过滤策略稳定，也能降低重写与翻译导致检测失效的脆弱性，同时避免对模型输出分布做直接操控带来的长期漂移与链路偏置累积。
其次，系统以显式行为分布 P_t 作为黑盒部署的证据载体，而不是依赖 logits、logprobs 与内部采样温度。在黑盒 API 场景中，系统通过提示词与工具协议要求 Agent 给出候选行为集合及其概率分布 P_t，再在 P_t 上进行分布保留采样。落地形态既支持提示词适配路径，使用 `agentmark\sdk\prompt_adapter.py` 强制输出 JSON 概率并鲁棒抽取 `action_weights`，也支持网关代理路径，使用 `agentmark\proxy\server.py` 以 OpenAI 风格 `POST v1 chat completions` 兼容方式拦截请求并在不改业务代码的情况下完成注入与采样。为了保证回放一致与跨端复现，系统对 P_t 做工程化规范，包括结构化输出、容错抽取、去重归一化与稳定排序，避免同概率项顺序变化导致编码与解码不同步。
最后，系统提供最小侵入的接入方式，允许业务把水印能力当作可开关的旁路能力使用。业务可选择配置级接入，将 Agent 的 `OPENAI_BASE_URL、base_url` 指向网关代理即可，也可在自研 Agent 中引入 `agentmark\sdk\watermarker.py` 的 `AgentWatermarker` 替换采样函数。为适配真实接入复杂度，系统支持网关形态、Sidecar 形态、浏览器插件形态、库内嵌 SDK 形态的切换，并给出一致的失败策略，在解析 P_t 失败或候选不完整时可预期降级并标记该步不可用于解码，保证业务链路不断。
进一步工程化补充是把能力做成可运营的产品特性。系统以协议兼容为第一原则，最大化复用 OpenAI 风格 `chat completions` 与 tools、functions 的既有字段，并把水印元数据放在 `extra_body.agentmark` 或独立 system 消息中，确保业务侧忽略 `watermark` 字段时主流程仍可照常运行。同时系统支持开关化与灰度化，能按会话、用户与流量比例启用或关闭，并通过 `session_id` 白名单、环境变量开关如 `AGENTMARK_TWO_PASS`、请求头如 `x-agentmark-session`、服务端策略下发等方式实现快速回滚。最后是多模型与多供应商可迁移性，网关层通过 `base_url、model` 映射完成适配，见 `agentmark\proxy\server.py` 中的 `_resolve_model`，算法层无需改动。
可度量验收指标建议聚焦三件事，接入成本控制在改一行 base_url、环境变量或替换一个采样函数，协议稳定性保证 `choices、message、content` 等关键字段结构在启用与关闭时一致，失败可回退保证 P_t 解析失败、候选缺失或网关不可用时自动降级并记录原因码。
产品化落地要点面向上线与运维主要覆盖部署拓扑、会话隔离、密钥载荷、关键配置、回滚隔离五个方面。
**部署拓扑，支持从实验到线上平滑迁移** 产品化系统通常存在三类典型部署拓扑，分别对应不同的组织边界与运维责任。共享网关 Shared Gateway 由一个 `AgentMark Proxy` 服务接入多个 Agent 与业务系统，集中注入与审计，适合监管侧或平台侧统一接入，优势是运维简单与策略统一，风险是成为集中瓶颈需要配套限流与弹性扩容。Sidecar 与同机代理 Per Service Sidecar 在每个业务服务或 Agent 服务旁挂一套代理，同机或同 Pod，业务侧 `base_url` 指向本地代理，优势是隔离强与故障影响面小，劣势是多实例运维与配置一致性成本更高。前端入口采用 Chrome 插件叠加内网网关适合 Web 场景，把会话标识、日志采集、上报队列前置到用户侧，后端只提供审计入口，优势是最小改造业务后端，代价是需要处理客户端网络与权限策略。
**会话与多租户隔离，避免串线导致误归因** 在产品形态下，系统必须显式管理 `session_id、conversation_id`，并确保每个会话的载荷、密钥派生上下文与水印状态相互隔离，bit index 与 round 也需独立管理。项目网关已实现会话级水印器缓存与 LRU 淘汰，见 `agentmark\proxy\server.py` 的 `_SESSION_CACHE` 与 `AGENTMARK_MAX_SESSIONS`，产品化时建议进一步固化。会话主键规范优先从请求头 `x-agentmark-session` 获取，其次从 `extra_body.agentmark.session_id` 获取，禁止在不可信字段上自动生成会话主键用于归因。多租户隔离策略可按租户与应用维度加前缀，例如 `tenantA_session123`，并在服务端做 ACL，避免不同租户读取彼此的审计数据。
**密钥与载荷管理，支持轮换、撤销与分环境隔离** 密钥不落代码要求共享密钥 K_{sh} 或其等价配置通过环境变量与密钥管理系统下发，禁止硬编码在仓库，测试环境与生产环境使用不同密钥域避免测试数据在生产侧被错误验证。载荷策略需要可配置，payload 可以是固定 bitstream，也可以是部署级标识映射后的 bitstream，例如 `deployment_id、owner_id、model_release` 等，项目支持 `AGENTMARK_PAYLOAD_BITS` 与 `AGENTMARK_PAYLOAD_TEXT` 两种入口，产品化建议纳入配置中心并支持定期轮换。轮换与兼容窗口要求在轮换密钥与载荷时保留兼容窗口，例如一段时间同时接受新旧 key 的验证，避免正在运行的长会话全部失效，Dashboard 侧应能显示 key version 并按版本验证。
**关键配置项，上线可直接对照** 下表给出上线常用的关键配置项，可作为评审与验收时的对照清单。
| 配置项 | 作用 | 线上建议 |
| --- | --- | --- |
| `DEEPSEEK_API_KEY` | 目标模型的调用密钥 | 走密钥托管、KMS，下发到网关实例 |
| `TARGET_LLM_BASE` | 目标 API Base URL | 通过配置中心管理，支持故障切换 |
| `TARGET_LLM_MODEL`、`TARGET_LLM_MODEL_MAP` | 模型名映射 | 兼容多模型，多版本灰度 |
| `AGENTMARK_TWO_PASS` | 是否启用 two-pass | 默认关闭，仅在候选不可得场景启用 |
| `AGENTMARK_PAYLOAD_BITS`、`AGENTMARK_PAYLOAD_TEXT` | 载荷配置 | 与租户、部署绑定，支持轮换与撤销 |
| `AGENTMARK_MAX_SESSIONS` | 会话水印器缓存上限 | 防内存膨胀，配合 LRU 淘汰 |
| `AGENTMARK_DEBUG` | 调试日志开关 | 生产默认关闭，避免泄露敏感信息 |
**回滚与故障隔离，保证水印挂了业务不能挂** 产品化系统必须把可用性优先级设计为业务回答优先，其次是水印注入，再是日志采集，最后才是监管可视化。旁路字段策略要求 `watermark` 字段永远可选，业务侧不依赖它做主逻辑，网关异常时允许直接回源调用目标 LLM。健康检查与熔断要求网关提供健康检查接口 `healthz` 或等价实现，业务侧与插件侧检测不可用时自动绕过，连续错误触发熔断并降级到 baseline。渐进降级要求高负载或异常时先关闭可视化 diff 与重计算字段，再关闭在线解码，最后关闭注入，把系统从全功能退化到透明转发。
### 2.1.2 鲁棒性，抗网络丢包与截断
目标是面对现实部署中不可避免的日志缺失与轨迹不完整，仍能恢复水印载荷并完成溯源验证。我们将该类失真抽象为两类常见现象。
一类是 Step Erasure，部分规划步骤丢失，可能来自日志未上报、平台侧过滤、采集失败等，另一类是 Trajectory Truncation，只观测到一段前缀或片段，可能来自链路中断、用户中止、回放不完整等。
在产品场景里，丢失与截断往往不是随机小概率事件，而是常态。浏览器与移动端断网、前端刷新、代理重启、上报队列溢出、平台合规删除、日志采样策略调整都可能造成规划链路不完整。若水印方案对齐完整轨迹与严格对齐的理想假设，一旦进入真实环境就会出现大面积不可解码。因此系统设计必须将残缺观测作为默认输入，围绕它构建解码与验证机制。
鲁棒性落地依赖三条主线，分别是可复现随机性、抗擦除聚合解码、采集链路的工程可靠性。
可复现随机性保证编码端与解码端同步，每一步使用共享密钥 K_{sh} 与上下文 Context_t 派生每步密钥 K_t，驱动伪随机过程，代码落点包括 `agentmark\core\watermark_sampler.py` 的 `DRBG` 与 `generate_contextual_key`，以及 `agentmark\sdk\watermarker.py` 与 `agentmark\proxy\server.py` 对 `context、history` 的显式传递。工程要点是上下文必须足够稳定，避免过度依赖可变自然语言文本导致跨端验证不同步，因此系统更倾向于使用结构化且可复现的上下文来源，将 `session_id、step_id、task_hash` 等稳定标识串联，叠加候选集合 B_t 的稳定哈希用于防替换，必要时只引入精简的历史摘要而非原始长文本。本项目采样实现也针对同步问题做了确定性工程，`watermark_sampler.py` 的 recombination 过程使用稳定排序，并对概率做离散化 rounding 来减少浮点噪声导致的同值排序不一致。
抗擦除能力来自在每步可变容量比特输出上叠加 RLNC 随机线性网络编码，把每个观测到的 bit 当作 GF2 上的线性方程，只要累积方程数足够且满秩，就可以从任意子序列恢复载荷。实现落点包括 `agentmark\core\rlnc_codec.py` 的 `DeterministicRLNC` 以及 `dashboard\server\app.py` 的演示流水线。与重复码与固定块码相比，RLNC 更适合任意子序列观测与可变容量码流的约束，丢失既可能随机发生也可能集中出现，固定块边界容易被破坏，而 `c_t` 又会随 bin 大小变化。产品侧通常同时支持单会话解码与全局聚合解码，前者用于取证场景，后者跨会话累积方程提升高丢失率场景的恢复成功率。
采集与重传策略把鲁棒性从算法可解提升到系统可用，在感知层通过本地队列叠加批量上报与幂等写入对抗网络抖动。客户端侧可用 WAL 与队列缓存实现断网可续并在恢复后批量上报，服务端侧以 `session_id、step_id、request_id` 做幂等键避免重传引起重复计数，系统也允许按轨迹片段做验证与解码以适配截断观测子集 I。工程上还需要把乱序与漂移显式纳入设计，日志到达可能乱序，应以 `step_id` 或单调计数器重排，时间戳只用于展示不参与对齐；候选集合可能随端侧版本变化，应记录候选哈希与版本号保证同一份 `B_t` 可回放验证；每一步还应记录是否可用于解码的原因码用于审计解释，并可选对关键字段 `P_t、b_wm_t、context` 的派生输入做签名或哈希链以增强防篡改。
**鲁棒性进一步扩展，从能解码到能作为监管证据**
观测缺失既可能来自网络抖动与系统故障，也可能来自攻击者的主动删改、刻意截断与挑选性隐藏。工程系统应默认对抗缺失存在，因此不能只追求平均成功率，还需要提供在最坏情况下仍可解释的证据链，例如在 Dashboard 中展示观测子集 I 的覆盖情况、缺失段位置、以及解码依赖的有效步骤集合，便于审计复核。
在语义保持重写与日志再构造场景下，行为分布 P_t 可能发生显著漂移，例如平均 KL 较大导致严格逐步同步不现实。系统层面可通过三类措施缓解，把 Context_t 绑定结构化状态如 session、step、候选哈希而不是长文本，把验证设计为多步聚合后的统计显著允许部分步骤不稳定，并在可视化上区分不可同步步与可同步步避免把自然漂移误判为攻击。
真实监管系统更关心错误归因成本，因此需要把鲁棒性扩展到抗伪造与误报控制，wrong key 情况应以高概率解码失败或显著性不足，无水印轨迹应呈现接近随机的比特一致性，通过验证的轨迹应给出置信度与统计量如 pvalue 或阈值，而不是只给成功失败二值结论。这也是应用层必须存在的原因，算法给出 bitstream，系统给出可解释结论。
**可度量验收指标，建议写进实验设置与系统指标**
验收指标建议覆盖抗擦除曲线，在不同丢失率下给出解码成功率随丢失率变化并对比单会话与全局聚合差异，覆盖截断可用性，在仅观测到前缀或中间片段时验证恢复概率随观测长度增长逐步提升，覆盖误报约束，在 wrong key 与无水印数据上验证通过率显著低于预设阈值并可通过统计检验控制。
**产品化鲁棒链路，日志管道与数据保全**
产品化鲁棒链路需要把算法能力变成可运维的证据管道，核心覆盖事件模型版本化、传输可靠性、数据保全合规、运行手册与排障四个方面。
**日志事件模型，Event Schema 版本化** 为了支撑长期运维与跨版本回放，建议把日志拆为若干稳定事件类型，并显式记录 schema 与 version。事件类型可包含 `llm_request` 记录请求元数据包含 session、step、context、候选哈希、模型信息、网关模式，`planning_distribution` 记录显式分布 P_t 与候选集 `B_t` 或其摘要并记录归一化方式与排序规则版本，`watermark_sample` 记录输出 `b_wm_t`、嵌入片段长度、round、bit_index 并可选携带可视化 diff，`execution_action` 记录工具调用名称与参数用于攻击链还原不参与解码但参与审计解释，`decode_result` 记录解码进度、恢复 payload、统计量如 pvalue、阈值、置信度与失败原因码。版本化的核心价值在于当算法或候选抽取策略升级时，旧日志仍可按旧版本规则复现与验证，避免升级后历史证据失效。
**传输可靠性，端侧队列叠加幂等落库与可观测丢失率** 抗丢包不仅是解码算法能恢复，也包括系统知道自己丢了多少与丢在哪里。端侧队列可观测要求记录队列长度、丢弃次数、最后成功上报时间并在必要时告警，服务端幂等要求用 `tenant、session_id、step_id、event_type、request_id` 做幂等键避免重试造成重复步骤，丢失率指标化要求把接收事件数与预期事件数比率、乱序率、重复率做成时序指标作为解码成功率的解释变量。
**数据保全与审计合规，监管侧可落地** 最小化原则要求默认不采集可识别个人信息 PII，对输入输出文本做脱敏与摘要，解码所需最小字段优先保留结构化数据 `P_t、b_wm_t` 与 context 摘要。权限与分级要求把业务运维可见数据例如性能与错误，与监管审计可见数据例如完整链路与解码证据分级授权，对外导出的证据报告记录导出人、时间与用途。不可抵赖性可选增强可对关键事件做签名或哈希链 hash chaining，并在 Dashboard 展示校验状态，使删改、插入、重排可被检测到。
**运行手册，Runbook 建议与解码失败定位** 当出现解码失败与显著性不足时，产品化排障建议优先检查系统性丢失，例如队列溢出、上报失败、服务端拒绝，再检查候选集或排序规则是否发生版本漂移，其次检查 context 派生输入是否稳定以及 session 与 step 是否一致，然后确认是否启用 two pass 导致分布波动增大，最后核对 key version 与轮换窗口是否同步以排除 wrong key。将这些原因固化为失败原因码并在 Dashboard 聚合统计，能把算法失败转化为可运维的问题定位。
### 2.1.3 用户无感，低延迟
目标是水印系统隐身在 Agent 流水线中，不显著增加端到端时延，不改变用户交互体验。
无感不是单一指标，而是三类角色在端到端链路中的共同体验，用户侧看不到额外提示与元信息，业务侧接入成本不增加，运维侧上线后资源与故障面可控且可回滚。
在实现层面，低延迟首先来自单步采样开销可控。分布保留采样只对候选行为概率做稳定排序与差分重组，并完成一次分类采样与 bin 内均匀编码，相比调用模型推理的开销几乎可以忽略。本项目候选规模通常较小，常见为 3 到 50 个工具与动作，采样成本稳定可预测，不会出现候选数膨胀导致的长尾延迟，并且采样发生在模型返回之后，不会增加模型侧 token 生成时间，用户体验接近一次常规后处理。
其次，系统把二次调用设计为可选兜底能力而不是默认路径。若候选集合可从 Agent 的 `tools` 与函数定义直接抽取，例如 `agentmark\proxy\server.py` 的网关路径，则走单次调用；只有在候选不可得且必须自举的黑盒场景才启用 two pass，并通过环境变量开关如 `AGENTMARK_TWO_PASS` 实现可控权衡。为了避免 two pass 对用户造成显著延迟，系统应配套候选缓存以复用同类任务候选列表，限制自举候选数量以避免后续分布退化与提示词膨胀，并对自举失败设置快速超时后直接降级到无水印透明转发路径。
最后，链路级优化把重计算与高风险操作全部移出在线快路径。网关代理优先做透明转发，把日志写入、可视化差异计算、RLNC 解码等放到异步任务中，确保用户侧响应不被阻塞。旁路数据采取分级策略，用户侧只需要最终 `content、tool_calls`，监管侧才需要 P_t、bin 分解与分布差异，因此可将可视化 diff 等重字段做成可选生成，并在高并发时仅保留最小必要字段。对概率分布等结构化字段可压缩并按时间窗合并上报，减少网络与存储压力，解码与统计检验也可采用延迟一致性策略，Dashboard 先展示轨迹后补齐解码结果，把体验优先级放在业务侧。
**端到端时延拆解，写作时可用作系统无感的论证框架**
将一次 Agent 决策的端到端耗时拆为四段。
i. 业务侧组包与发送；ii. 模型推理与网络往返；iii. 水印采样与结构化封装；iv. 日志上报与落库。
其中 ii 通常是主导项，iii 应被设计为常数级后处理，iv 必须严格旁路异步化。评估维度也包括 per step latency difference Δs、step、token、step 变化率，因此在系统描述里，我们可以明确一点，水印逻辑不应引入额外的模型推理调用，除非作为 two pass 兜底，也不应显著增加 token 开销，否则会直接抬升推理成本并影响用户体验。
**用户无感的隐蔽性侧面，避免被用户与模型显式察觉**
隐蔽性要求对用户界面无显式暴露，水印元数据不进入 `content`，避免用户看到概率与候选列表等内部信息，需要展示时由 Dashboard 在监管侧单独呈现。对模型输出也要保持最小干预，提示词注入采用独立 system 消息，并在工具调用场景尽量通过结构化 `tools` 候选抽取来减少对模型行为的额外约束，降低提示词变长导致风格变化的风险。对业务方同样保持无额外负担，业务系统不需要处理新字段即可运行，只有需要审计能力时才读取 `watermark` 扩展字段或对接 Dashboard。
**可度量验收指标，建议写进系统实现与实验报告**
验收指标建议覆盖三类可量化结论，时延维度统计启用与关闭水印时的 `Δs、step` 并按 P50、P95 展示，同时区分单次调用与 two pass 的差异；成本维度统计 token 与 step 的相对变化 `ΔTok、step`，确保注入不会造成明显 token 膨胀；体验维度在同一任务集上对比任务成功率与平均步骤数，确保无感不仅是快，还包括不中断与不降效。
**产品化性能与体验保障，把低延迟做成可运营能力**
产品化上线需要把低延迟从口号变成可运营能力。性能预算建议与 SLO、SLA 对齐，明确网关引入的额外延迟目标，例如单次调用模式下 `P95 < X ms、step`，two pass 只对少量会话启用并单独统计其 P95 与占比，同时把代理侧超时设为业务端到端超时的一定比例，例如三分之一，避免把单点波动放大为级联超时。高并发场景下还需要基础资源治理，按租户与会话限流以避免单一会话异常重试拖垮网关，设置并发上限与队列策略以在超过阈值时快速失败并触发业务侧绕过，配合候选集合、工具定义摘要与模型映射的缓存复用减少重复计算。
在故障与高负载下，系统应采用分级降级而不是随机失败。建议把功能拆为四级并可动态切换，Level 0 启用注入、采样、结构化日志、可视化差异与在线解码，Level 1 关闭可视化差异与重计算字段但保留最小 P_t 与 `b_wm_t`，Level 2 关闭在线解码并把解码转为离线与准实时批处理，Level 3 关闭注入并透明转发以业务优先。任何降级都不应改变业务最终输出结构，保证用户侧无感，同时必须记录降级等级与原因码以便审计侧正确解释。
可观测性闭环是把体验问题快速定位到具体环节的关键。指标建议拆解为请求转发耗时、模型端往返、水印采样耗时、日志写入耗时，并按错误码聚合解析失败、候选缺失、上游 429、5xx、超时、熔断等信号，再把任务成功率与平均步数同是否启用水印、是否 two pass、是否降级联动分析，确保无感不是以牺牲效用换来的。
---
## 2.2 系统总体架构
### 2.2.1 分层架构总览
系统采用三层叠加一条审计旁路 Audit Plane 的总体架构。数据面保证 Agent 正常完成任务，审计旁路保证溯源可验证、可观测、可追责。
在产品形态下，我们将整套系统明确拆为两条互相解耦的链路，避免为了监管而影响业务体验。
数据面 Data Plane 负责任务完成与用户体验，覆盖请求转发、候选抽取、分布采样、工具调用结果回传，该链路以低延迟与可用性为最高优先级，允许在异常时降级为透明转发。审计面 Audit Plane 负责可验证与可追责，覆盖结构化日志采集、解码恢复、统计检验、证据链生成与可视化，该链路以证据完整性与可解释性为最高优先级，允许延迟一致性并支持先出结果后补齐解码。
为确保跨组件协同一致，系统在分层之外还定义了统一的数据对象与接口契约。这些字段在网关、插件、Dashboard 的不同模块中复用，使注入、采样、采集、解码、展示形成闭环。
统一数据对象包括会话对象 Session，字段包含 `tenant_id` 可选、`session_id`、`user_id_hash` 可选、`payload_id`、`key_version`、`start_time`、`client_type` 可取插件、服务端、SDK；步骤对象 Step，字段包含 `step_id` 单调递增、`request_id` 幂等、`context` 结构化派生输入、`model`、`base_url`、`mode` 可取 single pass、two pass；规划证据 Planning Evidence，字段包含候选集合 `B_t` 或摘要哈希、显式分布 P_t 的归一化概率字典、水印选择 `b_wm_t`、嵌入片段元信息 `bits_embedded、bit_index、round_num`；执行证据 Execution Evidence，字段包含工具调用 `tool_name`、参数 `tool_args`、工具返回 `tool_result_digest` 并支持摘要与脱敏，同时记录错误码与重试信息；审计结论 Audit Result，字段包含恢复 payload、显著性统计量如 pvalue、阈值、置信度、失败原因码如丢失、漂移、错 key、不可同步，以及证据链导出元信息。
从安全边界视角看，三层分别承担不同的信任与隔离责任。
感知层更靠近用户与业务入口，强调最小采集、可控上报、隐私脱敏，算法层是可信计算内核，强调确定性、可复现、版本一致，应用层面向监管与运维，强调权限分级、证据留存、可解释展示。
本节配套黑白架构图已生成，TikZ 源文件见 `项目文档\fig_2_2_architecture.tex`，导出文件见 `项目文档\fig_2_2_architecture.pdf` 与 `项目文档\fig_2_2_architecture.png`。
### 2.2.2 感知层，Chrome 插件为核心工程量
感知层的定位是“**可部署、可控、可观测**”的前置入口，负责把水印能力嵌入到真实用户与真实 Agent 的交互链路里。
在本项目中，感知层并不限定为单一实现，既可以使用网关形态作为统一入口，直接复用 `agentmark\proxy\server.py` 适配大多数服务端与 Agent 框架，也可以使用 Chrome 插件形态把会话标识、请求拦截、日志采集与上报前置到浏览器侧，还可以在业务可改代码的场景采用 SDK 形态，在决策模块内部直接调用采样器获得更强控制力。
为了便于上线落地，Chrome 插件建议采用模块化能力拼装的方式实现，把 Interceptor、Policy Router、Session Manager、Injection Engine、Local Buffer、Uploader、Privacy Guard 等能力拆分清晰，使拦截与路由走同步快路径，上报与存储走异步旁路，同时与网关协同保持一致的会话与证据语义。
为了把插件形态写成可落地的工程方案，下面给出一个推荐的插件内部组件关系图，可直接用于答辩展示与项目文档。其核心思想是拦截与路由走同步快路径，上报与存储走异步旁路。
本节配套黑白组件图已生成，TikZ 源文件见 `项目文档\fig_2_2_extension_components.tex`，导出文件见 `项目文档\fig_2_2_extension_components.pdf` 与 `项目文档\fig_2_2_extension_components.png`。
**拦截与路由**
拦截对象面向浏览器内运行的 Agent，例如 Web IDE、Agent 控制台、业务管理后台，重点覆盖其对 LLM API 的 `fetch、XHR、WebSocket` 调用。路由策略将请求透明转发至本地或内网 `AgentMark Proxy`，或直接在插件侧做轻量注入后转发至原始 API，具体取决于部署策略。
产品化实现时，拦截与路由层需要特别处理三类真实世界复杂性，否则容易出现漏采或误采。
复杂性主要来自多端点与多供应商并存，流式响应与分片传输，以及跨域与鉴权三类问题。同一业务可能同时调用多个 LLM 端点，例如 OpenAI、DeepSeek、自建网关，路由策略需要白名单叠加版本化配置并支持动态更新。SSE 与 streaming 场景下响应分片到达，插件应在不破坏流式体验前提下做旁路采集，例如只采集首尾片段或结构化字段。跨域与鉴权要求正确处理 CORS、cookie、Authorization 头的透传与最小化暴露，避免引入额外安全风险。
进一步的产品化建议是把路由策略做成可运营策略，便于监管与运维在不发版的情况下调整。
可运营策略建议包括策略配置中心化，按域名、路径、模型名、环境等维度下发，环境维度可包含 dev、prod；灰度维度齐全，支持按用户哈希、session、时间窗、流量比例启用；安全默认拒绝，对未知域名与未知路径默认不拦截、不采集避免误采集敏感系统流量；旁路采样率支持按会话与步骤采样，例如仅采集 P_t 与哈希而不采集全文以控制合规与成本。
**水印注入，不破坏业务协议**
请求侧注入在 system 消息中加入输出候选行为及概率的约束指令，对应 `agentmark\sdk\prompt_adapter.py` 中的 `PROMPT_INSTRUCTION`，并附带 agentmark 元数据如 `session_id、context、round_num` 与候选集哈希。响应侧采用旁路字段策略，不修改原始 `content`，只追加 `watermark` 扩展字段，`agentmark\proxy\server.py` 的设计目标是保持后向兼容。
注入策略的产品化目标是两不一要，不破坏业务协议、不暴露敏感信息、并且要可回放，具体体现在
注入位置固定，优先新增独立的 system 消息放在 messages 最前且不覆盖业务原有 system 指令。元数据结构化，agentmark 元数据采用固定 JSON schema，例如 `session_id、context、key_version、mode`，避免将不可控自由文本作为同步输入。候选集合优先复用 tools，当业务已传入 `tools、functions` 时优先从中抽取候选动作以减少提示词膨胀与行为扰动。旁路字段约束要求 `watermark` 扩展字段可选且不影响业务侧解析，业务系统无需升级即可接收并忽略它。
同时需要明确注入会改变什么与不改变什么，便于上线评审。
不改变的部分包括业务方工具定义、业务系统对 `content、tool_calls` 的解析逻辑，以及最终用户可见输出文本。可能改变的部分主要是 LLM 的思考与自报概率输出格式，因为要求输出 JSON，同时 system prompt 的 token 数也会变化需要纳入 token 成本预算。可控改变的部分通过单次调用路径优先从 `tools` 抽取候选，将自报概率约束压缩为最小指令，并对 two pass 场景设置严格启用条件与超时。
在 Chrome 插件场景下，注入还需要遵守浏览器安全边界与隐私要求。
权限最小化要求只申请目标业务域名的 host 权限与必要的 storage 权限。敏感头处理要求对 Authorization 与 cookie 等敏感信息不落盘、不上报，仅用于透传。日志默认脱敏要求对 prompt 与 response 文本默认做摘要或脱敏，例如保留结构化字段、哈希或前 N 字符，避免把用户内容带入审计系统。
**行为日志采集**
采集最小完备集，使得解码与审计可复现，并对齐验证端输入 `b_wm_t、P_t`。
字段建议包括 `session_id、request_id、step_id、timestamp`，候选集合 `B_t` 或其哈希与概率分布 P_t，采样输出规划行为 `b_wm_t` 与嵌入比特 `m_t` 可选并记录用于复现的 `context`，执行动作如 tool call 及其参数用于攻击链还原，以及失败、重试、超时等异常状态用于鲁棒性统计。
为支持产品化审计与运维，建议在上述最小字段之外补充可观测性字段，把线上问题可快速定位为工程原因而非算法原因。
可观测性字段可包含 `route_path` 可取直连、网关、绕过，`degrade_level` 表示是否降级及级别，`parse_status` 表示分布解析是否成功，`candidate_source` 可取 tools、candidates、bootstrap，`schema_version` 标识日志、排序、解码版本，`upstream_status` 记录上游 429、5xx、timeout 并携带 `retry_count` 与 `queue_size` 表示端侧队列长度。
这些字段可以不参与解码，但会显著降低后续排障成本，并为“质量与合规报表”提供数据基础。
为了把日志采集变成可上线的采集系统，还需要定义上报协议与本地落盘策略，否则容易在高并发、断网、刷新下丢失证据。
上报协议与落盘策略建议包含事件粒度按 step 生成 2–5 条事件例如 request、distribution、sample、execution、可选 decode 避免大包失败，幂等键强制携带 `request_id` 与 `step_id` 并在服务端 upsert 保证重试不重复计数，批量与压缩按时间窗如 1–3 秒或条数阈值批量上报并 gzip 压缩，本地保留期为 IndexedDB 的 WAL 设置最大条数、最大容量与 TTL 例如 7 天或 500MB 并按最旧优先淘汰，安全传输要求上报通道走 HTTPS 并在必要时加签使用 HMAC 避免伪造事件注入审计面。
### 2.2.3 算法层，分布保留采样与鲁棒编码
算法层对齐 AgentMark-F 逻辑，并结合本项目实现做工程化拆分。
从产品化视角，算法层需要同时满足三件事，在线路径足够轻量不能成为数据面的瓶颈，离线路径足够鲁棒能在缺失与漂移下给出可解释结论，实现足够确定性使得同一证据在不同时间与不同机器上验证结果一致。
因此我们将算法层拆为四个子模块叠加两种运行模式，并与代码结构一一映射。
四个子模块的职责边界尽量清晰，便于工程回归与跨版本复现。分布获取与规范化 Distribution Elicitation and Canonicalization 负责从模型输出或工具定义得到候选集合与 P_t，完成去重、归一化、稳定排序与版本标记，代码落点包括 `agentmark\sdk\prompt_adapter.py` 的 JSON 抽取与容错，以及 `agentmark\proxy\server.py` 的候选抽取与模式切换。分布保留采样 Distribution Preserving Sampling 与 DPS 负责对 P_t 做 differential recombination、bin 采样、cyclic shift 编码，输出 `b_wm_t` 与嵌入片段元信息，代码落点包括 `agentmark\core\watermark_sampler.py` 与 `agentmark\sdk\watermarker.py`。鲁棒编码与聚合解码 Erasure Resilient Coding 与 Aggregation 负责将 variable capacity 的嵌入片段视为线性测量，在任意子序列下聚合恢复 payload，代码落点包括 `agentmark\core\rlnc_codec.py` 以及轻量 ECC 的 `agentmark\core\coding_utils.py`。验证与结论生成 Verification and Reporting 负责把解码结果转化为可解释结论，给出通过、不通过、不确定，并输出失败原因码供 Dashboard 展示与告警，这部分在本项目演示流水线中已具备基础解码与轨迹对比能力，产品化可继续完善。
两种运行模式分别面向不同链路目标。
在线采样模式 Online Sampling 在每一步返回前完成 `b_wm_t` 的选择与旁路元数据封装，必须是常数级后处理且失败可降级。离线与准实时验证模式 Offline Verification 从日志流中重放 `b_wm_t、P_t` 并聚合解码，允许延迟与批处理，强调证据完整与可解释。
为了支撑产品上线，算法层还需要具备配置化与可回归的工程特性，避免每次调整都成为不可控实验。
算法参数配置化覆盖 two pass 开关、候选数量上限、概率 rounding 精度、是否输出可视化 diff、RLNC 载荷长度与冗余策略等，这些参数应进入配置中心并写入日志便于事后复现。版本回放能力要求采样、排序、解码逻辑迭代后仍能按当时版本重放验证，避免升级导致历史证据失效。回归测试集应覆盖等概率、极端尖峰、长尾分布等固定样例，对编码与解码一致性做自动化回归，防止小改动破坏确定性。
**规划行为通道建模 Planning Time Behavior Channel**
每一步的规划行为候选集合记为 B_t，隐式策略记为 \pi_\theta(\cdot \mid o_t,h_t)，系统通过提示与协议得到显式近似分布 P_t。无水印基线记为 b_t \sim P_t，带水印采样输出 \hat{b}_t，可写作 \hat{b}_t \leftarrow Enc(P_t, m, r_t) 并满足逐步边际保持 \Pr[\hat{b}_t=b\mid P_t]=P_t(b)。
**Distribution Preserving Sampling 与 DPS**
对应 `agentmark\core\watermark_sampler.py` 的 differential 方案。
实现包含三个关键环节，先用 Differential recombination 对排序后的概率序列做一阶差分，把原分布分解为多个均匀 bin 的混合并在 bin 上按混合权重采样，再用 CyclicShift uniform encoding 在选中 bin 内做均匀编码，将 payload 比特嵌入到 bin 内元素索引的选择中，从而在不改变边际的情况下承载信息，最后用 Keyed PRG 以共享密钥与上下文生成每步可复现随机数，保证编码与解码一致。
**Erasure Resilient Coding 抗擦除编码**
考虑到每步可嵌比特数 `c_t` 受 bin 大小影响而变化，呈现 variable capacity 形态，系统采用 rateless 风格编码。
编码端将 payload `m` 映射为无限码流或足够长码流，每步消耗 `c_t` 个码流比特并嵌入到该步行为选择中，解码端从观测到的任意子序列 I 聚合得到总接收比特数 `R=∑_{t∈I} c_t`，只要线性系统满秩即可恢复 `m`。
本项目除 RLNC 外，还保留了轻量 ECC，如 parity、hamming，见 `agentmark\core\coding_utils.py`，用于小载荷或弱丢包场景的快速校验。
**产品化关键点，确定性与版本一致**
在线上系统中，同一日志在不同机器上解码不一致会直接破坏监管可信度，因此算法层必须显式处理确定性问题。
排序确定性要求概率相等或浮点误差很小时仍保持稳定结果，本项目在 recombination 中使用稳定排序并做 rounding 来减少噪声影响。候选一致性要求候选集合 `B_t` 以稳定方式序列化与哈希并记录来源与版本，来源可取 tools、candidates、bootstrap，避免同一步候选集不同导致不可解码。上下文确定性要求 `context` 绑定结构化字段如 session、step、候选哈希，避免绑定可变自然语言，密钥轮换需记录 `key_version`。实现版本化要求 `schema_version、sampler_version、decoder_version` 显式标记，确保历史日志可按旧版本规则回放验证。
此外，产品化系统需要把算法失败转化为可定位的失败原因，以支撑运维告警与工单流转。
常见原因包括分布不可用，表现为 P_t 解析失败、候选缺失、概率全为 0、NaN 或候选数量小于最小阈值，同步不可用，表现为 `session_id、step_id、context` 不一致、key version 不匹配或候选哈希漂移，证据不足，表现为丢失率过高导致 RLNC 方程不足或不满秩，上游异常导致步骤缺失，以及策略降级导致部分字段未生成。
将这些原因固化为 `reason_code` 并贯穿日志与 Dashboard，可显著提升“线上可解释性”，也能避免监管人员把系统故障误判为攻击。
#### 2.2.4 应用层，Vue3 Dashboard 监管大屏
应用层面向监管、审计、攻防复盘三类需求提供可视化能力。
能力覆盖会话级视图与水印解码视图，按 `session_id` 展示任务、步骤、候选集合、分布变化与采样结果，并展示 bitstream 恢复进度、成功率、置信度、统计量如 pvalue，同时提供攻击链还原与告警报表，把规划行为与执行动作串联成可追溯链路并定位可疑工具调用与关键决策点，对高丢包、异常截断、疑似篡改、伪造 key 等异常模式支持阈值告警与导出。
从产品化角度，Dashboard 不只是展示页面，更是审计面的控制台。它需要把算法输出转化为可操作的监管工作流，并提供面向运维的可观测与排障入口。结合本项目已有实现 `dashboard`，应用层可拆为后端服务、前端可视化、数据存储三部分。
**后端服务，FastAPI**
后端负责会话与场景管理、流式步骤回传，以及可选的解码验证与指标聚合。本项目后端入口为 `dashboard\server\app.py`，已具备以下产品化雏形能力。
后端接口覆盖会话初始化与自定义初始化，实时步进与 NDJSON 流式输出，会话恢复与续写，效果评估，场景持久化与检索，分别对应 `POST api init`、`POST api init_custom`、`POST api step`、`POST api restore_session`、`POST api continue`、`POST api evaluate`、`GET api scenarios`、`POST api save_scenario` 等路径。
产品化建议是在此基础上补齐鉴权、租户隔离、审计导出接口，并把失败原因码、降级等级、丢失率指标作为第一类字段进行聚合展示。导出接口可覆盖 JSON、CSV、PDF。
此外，后端服务建议补齐一组监管工作流接口，使 Dashboard 从演示页面升级为审计控制台。
监管工作流接口可覆盖溯源验证与统计检验，按时间窗与租户的批量扫描，证据包导出，策略下发等能力，其中溯源验证可按 `session_id` 或 `payload_id` 触发，批量扫描覆盖高截断、高丢失、异常工具调用等风险模式，证据包导出包含最小必要字段、哈希链、签名状态与关键步骤摘要，策略下发覆盖白名单、采样率、脱敏级别、降级阈值。
**数据存储，SQLite 可平滑扩展**
当前实现使用 SQLite，见 `dashboard\server\database.py` 保存会话与场景，适合单机演示与小规模部署。产品化落地时可按规模平滑迁移。
小规模可使用 SQLite 叠加定期备份，适合 PoC 与单机监管台，中规模可迁移到 PostgreSQL、MySQL 支持并发写入、权限与审计日志，大规模可采用日志流与 OLAP 组合，例如 Kafka、Pulsar、ClickHouse，用于高吞吐审计与报表。
无论存储选型如何，建议保留事件模型版本化与幂等键，确保日志在迁移与重放时仍可验证。
**前端可视化，Vue3**
前端的产品化目标是让监管人员在 1–2 分钟内回答三个问题，这是谁的 Agent、做了什么、证据是否可靠。
核心交互建议以时间线叠加对比视图为主，baseline 与 watermarked 并排对比突出 `P_t、b_wm_t` 与 tool calls 的差异点，同时提供解码进度与证据面板展示 bitstream 恢复进度、满秩条件、显著性结论与原因码，并在 step 粒度标记不可同步、解析失败、候选漂移、上游错误、降级等事件支持跳转原始日志，最后提供导出与报表能力用于合规留存与人工复核。
为了满足产品化的可控可管，前端还应承担 RBAC 权限分级与合规提示能力。
RBAC 分级建议让运维人员只看到性能、错误与脱敏后的摘要，让审计人员可查看结构化证据，让管理员可执行策略下发与导出证据包。字段脱敏标识建议在界面上明确展示脱敏方式与级别，例如哈希、摘要、截断，避免审计人员误以为证据缺失。导出前确认建议在导出证据包前提示包含字段范围与用途，并记录导出操作日志，包含导出人、时间、会话范围。
最后，产品化监管系统需要可告警而不仅是可展示。建议把关键规则固化为告警模板，并在 Dashboard 中支持配置。
告警模板可覆盖证据风险告警，包含丢失率超过阈值、连续截断、schema version 漂移、key version 不一致等模式，也可覆盖行为风险告警，围绕高危工具调用并按 allowlist、denylist、异常频率调用、参数异常模式等规则触发，还可覆盖系统健康告警，围绕上游 429、5xx 升高、网关熔断频繁、端侧队列长期堆积、导出失败等信号触发。
这些告警与前面的“失败原因码”相配合，能够把监管系统从“事后分析”升级为“在线巡检与准实时预警”。
---
## 2.3 业务流程设计
本节给出两个核心流程图，水印注入流程对应在线链路，溯源提取流程对应离线与准实时审计链路。
与传统算法模块描述不同，本节以产品落地为目标，明确以下三点。
本节重点回答三件事，用户、业务 Agent、插件、网关、算法内核、审计服务分别承担什么职责并交换哪些输入输出，在线链路如何在低延迟前提下不影响业务并在异常时可降级，离线与准实时链路如何把日志转化为可验证结论叠加可解释证据并支撑告警、复核与导出。
为支撑两条链路协同，系统对会话与步骤定义了最小状态机，建议写入实现规范，便于研发与验收一致。
本节配套黑白状态机图已生成，TikZ 源文件见 `项目文档\fig_2_3_state_machine.tex`，导出文件见 `项目文档\fig_2_3_state_machine.pdf` 与 `项目文档\fig_2_3_state_machine.png`。
其中 `Running` 状态的每个步骤建议进一步细分为 `REQUESTED -> SCORED_Pt -> SAMPLED_bwm_t -> EXECUTED -> LOGGED`，以便在流式场景下逐段上报与排障定位。
### 2.3.1 水印注入流程图，在线链路
本节配套黑白注入流程图已生成，TikZ 源文件见 `项目文档\fig_2_3_injection_flow.tex`，导出文件见 `项目文档\fig_2_3_injection_flow.pdf` 与 `项目文档\fig_2_3_injection_flow.png`。
**流程要点，工程约束**
在线链路有三条硬约束，候选集合来源优先级遵循 `tools、functions` 优先，其次显式 `candidates`，最后才是模型自举生成 bootstrap，响应侧必须保证 `content` 不被语义修改，水印信息只进入旁路字段，日志侧必须保留解码所需最小字段，至少 `b_wm_t` 与 P_t，并记录用于 PRG 同步的 `context`。
在线链路的产品化实现可以按三段理解。第一段完成会话识别与幂等标识，生成 `session_id、step_id、request_id` 并记录 `route_path`，同时抽取候选集合 `B_t` 并生成稳定的 `candidate_hash`，候选缺失时进入 two pass 自举或直接降级为 baseline 不加水印。第二段完成分布获取与解析，从模型输出抽取 `action_weights` 得到归一化 P_t 并记录 `parse_status`，解析失败时使用保守退化分布继续采样但标记不可用于解码，然后派生结构化 `context` 并执行 DPS 采样得到 `b_wm_t` 与嵌入片段元信息如 bits_embedded、round_num、bit_index，可选生成可视化 diff。第三段保证业务执行与审计旁路互不影响，业务侧按原协议执行 tool calls 并推进任务，日志侧异步上报 step 事件并通过 WAL、批量重试、服务端幂等写入保证可靠性，当出现上游 429、5xx、超时或队列堆积时按 `degrade_level` 与熔断状态逐级降级，优先关闭重计算字段与在线解码，最终可退化为透明转发。
**在线链路输入与输出清单，便于写作与验收**
| 环节 | 关键输入 | 关键输出 | 必须落日志 |
| --- | --- | --- | --- |
| 候选抽取 | `tools、functions` 或 `candidates` | `B_t`、`candidate_hash`、`candidate_source` | 是 |
| 分布解析 | 模型输出 JSON | P_t、`parse_status`、`normalize_rule` | 是 |
| 采样封装 | P_t + `context` | `b_wm_t`、`bits_embedded`、`round_num` | 是 |
| 执行回传 | `b_wm_t` | tool_calls、最终答案 | 否，但建议记录摘要 |
| 上报落库 | step 事件 | 事件入库结果、幂等确认 | 是 |
### 2.3.2 溯源提取流程图，离线与准实时
本节配套黑白溯源流程图已生成，TikZ 源文件见 `项目文档\fig_2_3_extraction_flow.tex`，导出文件见 `项目文档\fig_2_3_extraction_flow.pdf` 与 `项目文档\fig_2_3_extraction_flow.png`。
**流程要点，鲁棒与安全**
离线与准实时链路要同时满足三类安全与鲁棒目标，对任意观测子集 I 的抗擦除与截断能力，错误 key 与随机轨迹在统计检验下高概率失败的抗伪造能力，以及跨会话聚合恢复的多源融合能力，global decoding 用于提升极端丢包下的恢复率。
离线与准实时链路的产品化实现同样可以按三段理解。第一段 ingest 与 normalize，把插件与网关上报的 step 事件流按 `tenant_id、session_id` 聚合成会话日志视图，完成幂等去重、按 `step_id` 重排、按 schema version 兼容填充并记录 `schema_mismatch`。第二段 decode 与 aggregate，对每一步 `P_t、b_wm_t、context` 做逐步解码得到 `m_t` 与 `step_decodable`，再在多个步骤甚至多个会话上聚合解码恢复 payload 并计算满秩条件与证据强度指标如收到方程数 R、有效步数。第三段 verification 与 evidence，把统计检验结果转化为 `PASS、FAIL、INCONCLUSIVE` 并给出 `reason_code`，随后生成证据包，内容包含最小必要字段、版本信息、哈希链、签名状态可选、关键步骤摘要与导出元数据，对 `INCONCLUSIVE` 或高风险会话进入人工复核队列并把复核结论回写审计库。
**准实时与离线，两种运行方式的产品化取舍**
准实时 Near Real Time 以会话为单位滚动聚合，每收到新 step 就更新解码进度条与风险评分，适合在线巡检与告警但对存储与吞吐要求更高。离线 Batch 按时间窗例如小时或天批处理，适合合规报表与历史取证，成本更低但时效性较差。两种方式可以并存，准实时用于早发现，离线用于最终出具证据报告。
**原因码到处置动作，运维与审计可直接执行**
| `reason_code` | 含义 | 优先处置动作 |
| --- | --- | --- |
| `insufficient_evidence` | 丢失、截断导致证据不足 | 尝试跨会话聚合；检查端侧队列与上报失败率 |
| `schema_drift` | 版本不一致导致不可回放 | 按 `schema_version` 回放旧逻辑；检查灰度发布是否混跑 |
| `context_mismatch` | session、step、context 不一致 | 排查会话串线、step_id 重置、或候选哈希漂移 |
| `wrong_key_or_forgery` | 错 key 或疑似伪造 | 核对 key_version、轮换窗口；触发安全复核与证据导出 |
| `upstream_failure` | 上游 429、5xx、超时 | 走业务侧重试、降级；同时降低该段证据权重 |
**攻击链还原，把结论变成可解释故事线**
当解码通过或进入人工复核时，系统应把规划行为、执行动作、外部影响串成时间线，帮助监管人员快速理解风险点。
系统可以从 `b_wm_t` 与 `tool_calls` 生成行为序列图并标记关键决策点，例如高危工具、异常参数、连续重试，同时把证据风险事件如缺失、漂移、降级叠加到时间线，避免把系统故障误判为攻击。为了支撑审计闭环，Dashboard 还应提供一键导出能力，导出内容包含摘要、关键步骤、对应的统计结论与原因码，便于留存与复核。